{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBITN0M_LKds"
      },
      "source": [
        "# HW2P2: Face Classification and Verification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NH4P-HzLRQs"
      },
      "source": [
        "Congrats on coming to the second homework in 11785: Introduction to Deep Learning. This homework significantly longer and tougher than the previous homework. You have 2 sub-parts as outlined below. Please start early! \n",
        "\n",
        "\n",
        "*   Face Recognition: You will be writing your own CNN model to tackle the problem of classification, consisting of 7000 identities\n",
        "*   Face Verification: You use the model trained for classification to evaluate the quality of its feature embeddings, by comparing the similarity of known and unknown identities\n",
        "\n",
        "For this HW, you only have to write code to implement your model architecture. Everything else has been provided for you, on the pretext that most of your time will be used up in developing the suitable model architecture for achieving satisfactory performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1B_m84_cU6c"
      },
      "source": [
        "Common errors which you may face in this homeworks (because of the size of the model)\n",
        "\n",
        "\n",
        "*   CUDA Out of Memory (OOM): You can tackle this problem by (1) Reducing the batch size (2) Calling `torch.cuda.empty_cache()` and `gc.collect()` (3) Finally restarting the runtime\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdoDIKWOMF59"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jza7lwiScUhb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Oct 12 23:28:54 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:0A:00.0  On |                  N/A |\n",
            "|  0%   44C    P8    25W / 215W |    376MiB /  8192MiB |      1%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1519      G   /usr/lib/xorg/Xorg                212MiB |\n",
            "|    0   N/A  N/A      1660      G   /usr/bin/gnome-shell               57MiB |\n",
            "|    0   N/A  N/A      3616      G   ...3/usr/lib/firefox/firefox      104MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # to see what GPU you have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwLEd0gdPbSc",
        "outputId": "b8988980-9fbe-4dd5-ed05-2f685d7fb1b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary\n",
        "import torchvision #This library is used for image-based operations (Augmentations)\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import glob\n",
        "import wandb\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif torch.backends.mps.is_built():\n",
        "    device = 'mps'\n",
        "else:\n",
        "    device = 'cpu' \n",
        "\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O68hT27SXClj"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S7qpMxG0XCJz"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'batch_size': 128, # Increase this if your GPU can handle it\n",
        "    'lr': 0.1,\n",
        "    'epochs': 25, # 10 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n",
        "    # Include other parameters as needed.\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSeiKHYrM-6b"
      },
      "source": [
        "# Classification Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tmRX5omaNDEZ"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '/home/jbajor/Dev/CMU-IDL/datasets/hw2p2/'# TODO: Path where you have downloaded the data\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, \"classification/train\") \n",
        "VAL_DIR = os.path.join(DATA_DIR, \"classification/dev\")\n",
        "TEST_DIR = os.path.join(DATA_DIR, \"classification/test\")\n",
        "\n",
        "# Transforms using torchvision - Refer https://pytorch.org/vision/stable/transforms.html\n",
        "\n",
        "train_transforms = torchvision.transforms.Compose([ \n",
        "    # Implementing the right transforms/augmentation methods is key to improving performance.\n",
        "                    torchvision.transforms.ToTensor() \n",
        "                    ])\n",
        "# Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()\n",
        "# But there are some transforms which are performed after ToTensor() : e.g - Normalization\n",
        "# Normalization Tip - Do not blindly use normalization that is not suitable for this dataset\n",
        "\n",
        "val_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR, transform = train_transforms)\n",
        "val_dataset = torchvision.datasets.ImageFolder(VAL_DIR, transform = val_transforms)\n",
        "# You should NOT have data augmentation on the validation set. Why?\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = config['batch_size'], \n",
        "                                           shuffle = True,num_workers = 4, pin_memory = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = config['batch_size'], \n",
        "                                         shuffle = False, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SqSR063BGE2e"
      },
      "outputs": [],
      "source": [
        "# You can do this with ImageFolder as well, but it requires some tweaking\n",
        "class ClassificationTestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir   = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This one-liner basically generates a sorted list of full paths to each image in the test directory\n",
        "        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fVLB41KtGC2o"
      },
      "outputs": [],
      "source": [
        "test_dataset = ClassificationTestDataset(TEST_DIR, transforms = val_transforms) #Why are we using val_transforms for Test Data?\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = config['batch_size'], shuffle = False,\n",
        "                         drop_last = False, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4t8eU9gY0Jy",
        "outputId": "6c93757b-7b48-4f1d-cc6c-524bd866c004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes:  7000\n",
            "No. of train images:  140000\n",
            "Shape of image:  torch.Size([3, 224, 224])\n",
            "Batch size:  128\n",
            "Train batches:  1094\n",
            "Val batches:  274\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of classes: \", len(train_dataset.classes))\n",
        "print(\"No. of train images: \", train_dataset.__len__())\n",
        "print(\"Shape of image: \", train_dataset[0][0].shape)\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Train batches: \", train_loader.__len__())\n",
        "print(\"Val batches: \", val_loader.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqmojPaWD0H"
      },
      "source": [
        "# Very Simple Network (for Mandatory Early Submission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ny-mh_ocWIJR"
      },
      "outputs": [],
      "source": [
        "from model import ResEXP\n",
        "from hparams import Hparams\n",
        "\n",
        "hparams = Hparams()\n",
        "\n",
        "model = ResEXP(hparams).to(device)\n",
        "# out = model.debug(next(iter(train_loader))[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZCn0qHuZRKj"
      },
      "source": [
        "# Setup everything for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UowI9OcUYPjP"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
        "# TODO: Implement a scheduler (Optional but Highly Recommended)\n",
        "# You can try ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n",
        "scaler = torch.cuda.amp.GradScaler() # Good news. We have FP16 (Mixed precision training) implemented for you\n",
        "# It is useful only in the case of compatible GPUs such as T4/V100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzM11HtcboYv"
      },
      "source": [
        "# Let's train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bgSw6iJJavBZ"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    # Progress Bar \n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5) \n",
        "    \n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        \n",
        "        optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it! \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # Update no. of correct predictions & loss as we iterate\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct,\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() \n",
        "\n",
        "        # TODO? Depending on your choice of scheduler,\n",
        "        # You may want to call some schdulers inside the train function. What are these?\n",
        "      \n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m5V2UdnpdEoK"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader, criterion):\n",
        "  \n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
        "\n",
        "    num_correct = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        \n",
        "        # Move images to device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct)\n",
        "\n",
        "        batch_bar.update()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cmotca6pcLLY"
      },
      "outputs": [],
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBgKGkXLrdJ"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ix62_BkaLr_D"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjbajor\u001b[0m (\u001b[33midl-group\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jbajor/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.13.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jbajor/Dev/HW2/wandb/run-20221012_232900-u8gngz6c</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/idl-group/hw2p2-ablations/runs/u8gngz6c\" target=\"_blank\">ResEXP_v1_micro_1665631740</a></strong> to <a href=\"https://wandb.ai/idl-group/hw2p2-ablations\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from utils import initiate_run\n",
        "\n",
        "run = initiate_run(hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG0vmsmbRYEi"
      },
      "outputs": [],
      "source": [
        "# # Create your wandb run\n",
        "# run = wandb.init(\n",
        "#     name = \"early-submission\", ## Wandb creates random run names if you skip this field\n",
        "#     reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "#     # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "#     # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "#     project = \"hw2p2-ablations\", ### Project should be created in your wandb account \n",
        "#     config = config ### Wandb Config for your run\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkRw1FvLqYe"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EqWO8Edb0BK2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train:   0%|          | 0/1094 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Given normalized_shape=[96], expected input with shape [*, 96], but got input of size[128, 96, 56, 56]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     curr_lr \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_acc, train_loss \u001b[39m=\u001b[39m train(model, train_loader, optimizer, criterion)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEpoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTrain Acc \u001b[39m\u001b[39m{:.04f}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Train Loss \u001b[39m\u001b[39m{:.04f}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m Learning Rate \u001b[39m\u001b[39m{:.04f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         config[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         train_acc,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         train_loss,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         curr_lr))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     val_acc, val_loss \u001b[39m=\u001b[39m validate(model, val_loader, criterion)\n",
            "\u001b[1;32m/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb Cell 26\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(): \u001b[39m# This implements mixed precision. Thats it! \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jbajor/Dev/HW2/HW2P2_Starter_F22.ipynb#X34sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Update no. of correct predictions & loss as we iterate\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Dev/HW2/model.py:173\u001b[0m, in \u001b[0;36mResEXP.forward\u001b[0;34m(self, x, headless)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, headless\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork:\n\u001b[0;32m--> 173\u001b[0m         x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m    174\u001b[0m     \u001b[39mif\u001b[39;00m headless:\n\u001b[1;32m    175\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/modules/normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    190\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
            "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.10/site-packages/torch/nn/functional.py:2503\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2499\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2500\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2501\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2502\u001b[0m     )\n\u001b[0;32m-> 2503\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[96], expected input with shape [*, 96], but got input of size[128, 96, 56, 56]"
          ]
        }
      ],
      "source": [
        "best_valacc = 0.0\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_acc, train_loss = train(model, train_loader, optimizer, criterion)\n",
        "    \n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        config['epochs'],\n",
        "        train_acc,\n",
        "        train_loss,\n",
        "        curr_lr))\n",
        "    \n",
        "    val_acc, val_loss = validate(model, val_loader, criterion)\n",
        "    \n",
        "    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n",
        "\n",
        "    wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc, \n",
        "               'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n",
        "    \n",
        "    # If you are using a scheduler in your train function within your iteration loop, you may want to log\n",
        "    # your learning rate differently \n",
        "\n",
        "    # #Save model in drive location if val_acc is better than best recorded val_acc\n",
        "    if val_acc >= best_valacc:\n",
        "      #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n",
        "      print(\"Saving model\")\n",
        "      torch.save({'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optimizer.state_dict(),\n",
        "                  #'scheduler_state_dict':scheduler.state_dict(),\n",
        "                  'val_acc': val_acc, \n",
        "                  'epoch': epoch}, './checkpoint.pth')\n",
        "      best_valacc = val_acc\n",
        "      wandb.save('checkpoint.pth')\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpgCHImRkYQW"
      },
      "source": [
        "# Classification Task: Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2WQEUjXkWvo"
      },
      "outputs": [],
      "source": [
        "def test(model,dataloader):\n",
        "\n",
        "  model.eval()\n",
        "  batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n",
        "  test_results = []\n",
        "  \n",
        "  for i, (images) in enumerate(dataloader):\n",
        "      # TODO: Finish predicting on the test set.\n",
        "      images = images.to(device)\n",
        "\n",
        "      with torch.inference_mode():\n",
        "        outputs = model(images)\n",
        "\n",
        "      outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()\n",
        "      test_results.extend(outputs)\n",
        "      \n",
        "      batch_bar.update()\n",
        "      \n",
        "  batch_bar.close()\n",
        "  return test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7R1lcCAzULc",
        "outputId": "ed1d1147-46d3-4a6c-832f-6dbb2b0bde8b"
      },
      "outputs": [],
      "source": [
        "test_results = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqfUzwS2L1gx"
      },
      "source": [
        "## Generate csv to submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vob9a2-HkW_V"
      },
      "outputs": [],
      "source": [
        "with open(\"classification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(test_dataset)):\n",
        "        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", test_results[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsJx1l1T4twC"
      },
      "source": [
        "# Verification Task: Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoBFFF8-Lpvj"
      },
      "source": [
        "The verification task consists of the following generalized scenario:\n",
        "- You are given X unknown identitites \n",
        "- You are given Y known identitites\n",
        "- Your goal is to match X unknown identities to Y known identities.\n",
        "\n",
        "We have given you a verification dataset, that consists of 1000 known identities, and 1000 unknown identities. The 1000 unknown identities are split into dev (200) and test (800). Your goal is to compare the unknown identities to the 1000 known identities and assign an identity to each image from the set of unknown identities. \n",
        "\n",
        "Your will use/finetune your model trained for classification to compare images between known and unknown identities using a similarity metric and assign labels to the unknown identities. \n",
        "\n",
        "This will judge your model's performance in terms of the quality of embeddings/features it generates on images/faces it has never seen during training for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9aY5o-suWdn",
        "outputId": "e641fa12-af84-411f-c21c-a70947518d4c"
      },
      "outputs": [],
      "source": [
        "known_regex = \"/home/jbajor/Dev/CMU-IDL/datasets/hw2p2/verification/known/*/*\"\n",
        "known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))] \n",
        "# This obtains the list of known identities from the known folder\n",
        "\n",
        "unknown_regex = \"/home/jbajor/Dev/CMU-IDL/datasets/hw2p2/verification/unknown_test/*\" #Change the directory accordingly for the test set\n",
        "\n",
        "# We load the images from known and unknown folders\n",
        "unknown_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_regex)))]\n",
        "known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]\n",
        "\n",
        "# Why do you need only ToTensor() here?\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()])\n",
        "\n",
        "unknown_images = torch.stack([transforms(x) for x in unknown_images])\n",
        "known_images  = torch.stack([transforms(y) for y in known_images ])\n",
        "#Print your shapes here to understand what we have done\n",
        "\n",
        "# You can use other similarity metrics like Euclidean Distance if you wish\n",
        "similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk1LS0BRxFHM",
        "outputId": "ad9694ee-1075-4709-b50d-1ce6a049dc14"
      },
      "outputs": [],
      "source": [
        "def eval_verification(unknown_images, known_images, model, similarity, batch_size= config['batch_size'], mode='val'): \n",
        "\n",
        "    unknown_feats, known_feats = [], []\n",
        "\n",
        "    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "    model.eval()\n",
        "\n",
        "    # We load the images as batches for memory optimization and avoiding CUDA OOM errors\n",
        "    for i in range(0, unknown_images.shape[0], batch_size):\n",
        "        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            unknown_feat = model(unknown_batch.float().to(device), return_feats=True) #Get features from model         \n",
        "        unknown_feats.append(unknown_feat)\n",
        "        batch_bar.update()\n",
        "    \n",
        "    batch_bar.close()\n",
        "    \n",
        "    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "    \n",
        "    for i in range(0, known_images.shape[0], batch_size):\n",
        "        known_batch = known_images[i:i+batch_size] \n",
        "        with torch.no_grad():\n",
        "              known_feat = model(known_batch.float().to(device), return_feats=True)\n",
        "          \n",
        "        known_feats.append(known_feat)\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    # Concatenate all the batches\n",
        "    unknown_feats = torch.cat(unknown_feats, dim=0)\n",
        "    known_feats = torch.cat(known_feats, dim=0)\n",
        "\n",
        "    similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])\n",
        "    # Print the inner list comprehension in a separate cell - what is really happening?\n",
        "\n",
        "    predictions = similarity_values.argmax(0).cpu().numpy() #Why are we doing an argmax here?\n",
        "\n",
        "    # Map argmax indices to identity strings\n",
        "    pred_id_strings = [known_paths[int(i)] for i in predictions]\n",
        "    \n",
        "    if mode == 'val':\n",
        "      true_ids = pd.read_csv('/home/jbajor/Dev/CMU-IDL/datasets/hw2p2/verification/dev_identities.csv')['label'].tolist()\n",
        "      accuracy = accuracy_score(pred_id_strings, true_ids)\n",
        "      print(\"Verification Accuracy = {}\".format(accuracy))\n",
        "    \n",
        "    return pred_id_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMC7FacaUnJ7"
      },
      "outputs": [],
      "source": [
        "pred_id_strings = eval_verification(unknown_images, known_images, model, similarity_metric, config['batch_size'], mode='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD-r-HmsAeWV"
      },
      "outputs": [],
      "source": [
        "with open(\"verification_early_submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(pred_id_strings)):\n",
        "        f.write(\"{},{}\\n\".format(i, pred_id_strings[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!kaggle competitions submit -c 11-785-f22-hw2p2-classification -f ./classification_early_submission.csv -m \"Early Submission\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!kaggle competitions submit -c 11-785-f22-hw2p2-verification -f ./verification_early_submission.csv -m \"Early Submission\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('dl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "10dcf7d0c74d3b5187da3e113dd39bd34402e63079129b42ab727bb0278d4ecf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
